{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Pre-Processing: \n",
    "Stopwords are removed in the tweets in order to improve the performance of classification using Naive Bayes Model At line 25 using a predefined stopwords.Here we pre-process the 30k data. \n",
    "\n",
    "Stopwords-One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "df = tweets = pd.read_excel(\"./clean_healthcare30000.xlsx\")\n",
    "df = df[['TweetText','Polarity']]\n",
    "stop_words = set(['a','about','above','after','again','against','all','am','an','and','any','are','aren\\'t','as','at','be','because','been','before','being','below','between','both','but','by','can\\'t','cannot','could','couldn\\'t','did','didn\\'t','do','does','doesn\\'t','doing','don\\'t','down','during','each','few','for','from','further','had','hadn\\'t','has','hasn\\'t','have','haven\\'t','having','he','he\\'d','he\\'ll','he\\'s','her','here','here\\'s','hers','herself','him','himself','his','how','how\\'s','i','i\\'d','i\\'ll','i\\'m','i\\'ve','if','in','into','is','isn\\'t','it','it\\'s','its','itself','let\\'s','me','more','most','mustn\\'t','my','myself','no','nor','not','of','off','on','once','only','or','other','ought','our','ours', 'ourselves','out','over','own','same','shan\\'t','she','she\\'d','she\\'ll','she\\'s','should','shouldn\\'t','so','some','such','than','that','that\\'s','the','their','theirs','them','themselves','then','there','there\\'s','these','they','they\\'d','they\\'ll','they\\'re','they\\'ve','this','those','through','to','too','under','until','up','very','was','wasn\\'t','we','we\\'d','we\\'ll','we\\'re','we\\'ve','were','weren\\'t','what','what\\'s','when','when\\'s','where','where\\'s','which','while','who','who\\'s','whom','why','why\\'s','with','won\\'t','would','wouldn\\'t','you','you\\'d','you\\'ll','you\\'re','you\\'ve','your','yours','yourself','yourselves'])\n",
    "\n",
    "def processRow(row):\n",
    "    tweet = row.lower()    #Lower case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)    #delete any url\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #delete any @Username\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)#Remove additional white spaces\n",
    "    tweet = re.sub('[\\n]+', ' ', tweet) #Remove not alphanumeric symbols white spaces\n",
    "    tweet = re.sub(r'[^\\w]', ' ', tweet) #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #Remove Digits\n",
    "    tweet = re.sub(\" \\d+\", '', tweet)\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    tweet = tweet.replace(':)','')    #Remove :( or :)\n",
    "    tweet = tweet.replace(':(','')\n",
    "    tweet = tweet.strip('\\'\"')    #trim\n",
    "    tweet = [word for word in tweet.split() if word not in stop_words and len(word) > 1]#Removes stopwords and single letter words\n",
    "    return ''.join(str(e) + \" \" for e in tweet)\n",
    "testing = np.array(list(df['TweetText'][:30000]))  \n",
    "for x in range(0,testing.shape[0]):\n",
    "    testing[x] = processRow(testing[x])\n",
    "df.TweetText = testing\n",
    "\n",
    "#df.to_csv(\"clean_stopwordsremoved_healthcaretweet30000.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: Naive Bayes (NB)/ Support Vector Machine (SVM)/ Logistic Regression (LR)\n",
    "This is the code used to get he results excel sheet to compare the performance of the three models in order to choose the fastest and most accurate. The model chosen (NB) as it is the fastest and performed the best in these two criteria.\n",
    "In order to use the code you should have the nltk and sklearn packages.<br>\n",
    "\n",
    "The Evaluation Metrics used to test the models performance are skllearns: precision, recall, fscore(f1), and accuracy.\n",
    "The Performance Metrics use to thes the models performance are in the terms of time taken to Build the Text Corpus, Vectorize the data and for the models to classify the overall record (15k/30k data).<br>\n",
    "\n",
    "Naming Concention of the csv files:<br>\n",
    "CSV files with stopwords:<br>\n",
    "    Version1: 15k data set<br>\n",
    "    clean_healthcaretweet750 - 750 labelled of 15k<br>\n",
    "    clean_healthcaretweet1500 - 1500 labelled of 15k<br>\n",
    "    clean_healthcaretweet2250 - 2250 labelled of 15k<br>\n",
    "    clean_healthcaretweet3000 - 3000 labelled of 15k<br>\n",
    "    Version2: 30k data set<br>\n",
    "    clean_healthcaretweet750_30k - 750 labelled of 15k<br>\n",
    "    clean_healthcaretweet1500_30k - 1500 labelled of 15k<br>\n",
    "    clean_healthcaretweet2250_30k - 2250 labelled of 15k<br>\n",
    "    clean_healthcaretweet3000_30k - 3000 labelled of 15k<br>\n",
    "\n",
    "CSV files without stopwords:<br>\n",
    "    Version1: 15k data set<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_15k - 750 labelled of 15k<br>\n",
    "    clean_stopwordsremoved_healthcaretweet1500_15k - 1500 labelled of 15k<br>\n",
    "    Version2: 30k data set<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_30k - 750 labelled of 30k<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_30k - 1500 labelled of 30k<br>\n",
    "Below is the code used to test different performance evaluations of the three models: test set=20%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set Random seed\n",
    "np.random.seed(500)\n",
    "\n",
    "\n",
    "def calPerformanceofModels(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "    start = time.time()\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Step - 1a : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    end = time.time()\n",
    "    tok_time = end-start\n",
    "    \n",
    "    # Step - 2: Split the model into Train and Test Data set\n",
    "    start  = time.time()\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "\n",
    "    # Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Step - 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    # Step - 5: Now we can run different algorithms to classify our data check for accuracy\n",
    "    end  = time.time()\n",
    "    vect_time = end-start\n",
    "    # Classifier - Algorithm - Naive Bayes\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "    end  = time.time()\n",
    "    nb_time = end -start\n",
    "    #NAIVE BAYES END\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "    end  = time.time()  \n",
    "    svm_time = end -start\n",
    "    #SVM END\n",
    "    \n",
    "    # Classifier - Algorithm - Logistic Regression\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time() \n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(Train_X_Tfidf,Train_Y)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions_LR = LogReg.predict(Test_X_Tfidf)\n",
    "    end = time.time()\n",
    "    lr_time = end -start\n",
    "    #lOGISTIC REGRESSION END\n",
    "\n",
    "    #We compute the precison, recall and fscore of each of the models\n",
    "    #We use Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for\n",
    "    #label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "    prf_NB = precision_recall_fscore_support(predictions_NB, Test_Y,average='weighted')\n",
    "    prf_SVM = precision_recall_fscore_support(predictions_SVM, Test_Y,average='weighted')\n",
    "    prf_LR = precision_recall_fscore_support(predictions_LR, Test_Y,average='weighted')\n",
    "    print(\"~~~~For N labels = \", label , \", Max features = \", max_features  ,\"~~~~~\\n\")\n",
    "    print(\"===Naive Bayes===\\nPrecision, Recall, F1-Score: \",  prf_NB[0]*100,prf_NB[1]*100,prf_NB[2]*100 )\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(Test_Y, predictions_NB)*100)\n",
    "    print(\"===SVM===\\nPrecision, Recall, F1-Score: \",  prf_SVM[0]*100,prf_SVM[1]*100,prf_SVM[2]*100)\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(Test_Y, predictions_SVM)*100)\n",
    "    print(\"===Logistic Regression===\\nPrecision, Recall, F1-Score: \",  prf_LR[0]*100,prf_LR[1]*100,prf_LR[2]*100)\n",
    "    print(\"LR Accuracy Score -> \",accuracy_score(Test_Y,predictions_LR )*100)\n",
    "    print(\"Time spent on tokenizing for bag of words: \", tok_time, \"\\n\")\n",
    "    print(\"Time spent on vectorizing for NB/SVM/LR\", vect_time, \"\\n\")\n",
    "    print(\"Time spent on predicting for NB/SVM/LR models (respectively): \", \"\\n\",\n",
    "          nb_time, \"/\", svm_time ,\"/\", lr_time,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Conducted: \n",
    "Variables being changed Max Features (number of important words in the text Corpus), Number of labelled Data and Total Number of Data (Scaling).<br>\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.<br>\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.<br>\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. In our example we used a multiplier of 100 so as we can read it clearer.<br>\n",
    "\n",
    "The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.<br>\n",
    "\n",
    "The support is the number of occurrences of each class in predictions_xxx.<br>\n",
    "\n",
    "If pos_label is None and in binary classification, this function returns the average precision, recall and F-measure if average is one of 'micro', 'macro', 'weighted' or 'samples'.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 (15k data set): There are Three variables that we vary to observe the performance of the three models: NB, SVM, LB \n",
    "For the first experiment we kept the Max Features and the total data the same.\n",
    "Changing the labelled Data.<br>\n",
    "\n",
    "Conclusion: Results is that as more labelled data is trained for classifying the 15k sets of tweets the evaluation metrics precision.recall/f1 and accuracy decreased significantly. However, a good effect is that the tokenizing time has decreased by roughly 5%. The time spent on vectorizing stayed roughly the same while classifying time for 15k data set has decreased as more labelled data is fed to it. \n",
    "\n",
    "Best to worst performing Model(evaluation and performance metrics): NB>>LB>>SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 (30k data set): There are Three variables that we vary to observe the performance of the three models: NB, SVM, LB \n",
    "For the third experiment we kept the Max Features and the total data the same.\n",
    "Changing the labelled Data.<br>\n",
    "Conclusion: Results is that as more labelled data is trained for classifying the 30k sets of tweets the evaluation metrics precision.recall/f1 and accuracy decreased roughly 4-7%. Moreover, the tokenizing time has increased roughly twice from 500s to 1000s. The time spent on vectorizing almost doubled as well. <br><br>\n",
    "Performance metrics Classification time For Models (increase of labelled data from 750 to 3000 of 30k):<br>\n",
    "NB: Decreased as labelled data increased<br>\n",
    "SVM: Increased 10x as labbeled data increased <br>\n",
    "LB: Stayed Roughly the Same<br>\n",
    "Classification time For Models (increase of labelled data from 750 to 3000 of 30k):<br>\n",
    "NB: Decreased as labelled data increased<br>\n",
    "SVM: Increased 10x as labbeled data increased <br>\n",
    "LB: Stayed Roughly the Same<br>\n",
    "\n",
    "Best to worst performing Model : NB>>LB>>SVM<br>\n",
    "In terms of time NB performed the fastest and the most accurate while LB/SVM performed around the same accuracy but worse than NB. SVM performed significantly slower than NB and LB and shouldn't used for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250_30k.csv',2250,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 (15k data set): Vary labelled data and Num of Max Features (MF), Keep total data the same.\n",
    "Changed the Labelled data from 750,1500,2250 and 3000 respectively.<br>\n",
    "Changed the Max Features from 2500,5000,7500 and 10000 respectively.<br>\n",
    "The data will load qite slow roughly 400s to 1000s per call of calPerformanceofModels:<br>\n",
    "\n",
    "Increasing MF increased the time it takes to tokenize, and vectorize the data for the same total number of data. From the test the MF value of roughly 5000 performed better thus 5000 was chosen for scaling.<br>\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR:Tokenizing and Vectorizing increased more than double<br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased x4 roughly<br>\n",
    "SVM:  Increased x4 roughly<br>\n",
    "LR: Increased x3 roughly<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 10%-15% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#750 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 10000)\n",
    "#1500 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 10000)\n",
    "#2250 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 10000)\n",
    "#3000 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 (30k data set): Vary labelled data and Num of Max Features (MF), Keep total data the same. Testing on Larger Data.\n",
    "Changed the Labelled data from 750,1500 and 3000 respectively.<br>\n",
    "Changed the Max Features from 2500,5000,7500 and 10000 respectively.<br>\n",
    "\n",
    "The difference between 15k and 30k data is that at 30k data the time taken to tokenize it stayed relatively the same as MF is increased which is good for scaling.\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR:Tokenizing and Vectorizing increased more than double<br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased x4 roughly<br>\n",
    "SVM:  Increased x4 roughly<br>\n",
    "LR: Increased x3 roughly<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 10%-15% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br><br>\n",
    "Takeaway: at scaling number of labelled data matters, a lower number might be better as biasedness in data might be better for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#750 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,10000)\n",
    "#1500 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,10000)\n",
    "#2250 labelled data, change MF \n",
    "## Validate Results using clean_healthcaretweet1500 dataset\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,10000)\n",
    "#3000 labelled data, change MF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 (30k data set): Effect of Removing Stopwords on Data, on the Three Models.\n",
    "Changed the Labelled data from 750,1500 and 3000 respectively.<br>\n",
    "Kept the Max Features at 5000.<br>\n",
    "\n",
    "The difference between 15k and 30k data is that at 30k data the time taken to tokenize it stayed relatively the same as MF is increased which is good for scaling.\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR: After stopwords has been removed time take to tokenize and has signigicantly decreased to more than half (860s to 320s) while time taken to vectorize data increased as the number of labelled data and after removing stopwords from (0.98s to 1.03s) which is not very big. <br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased (from 0.0059s to 0.0069) as labelled data increased. Thus it is better to keep labelled data a bit smaller. <br>\n",
    "SVM/LR:  For time to classify data both these models stayed almost the same with/without stopwords<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 4-7% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br><br>\n",
    "\n",
    "\n",
    "Takeaway: Throughout all the test NB performed the best in terms of both time and performance and in terms of possible scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#750\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "#1500\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet1500_30k.csv',1500,5000)\n",
    "#3000\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet3000_30k.csv',3000,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes scaling from 15k to 30k removing Stopwords\n",
    "Kept the Labelled data at 750.<br>\n",
    "Kept the Max Features 5000.<br>\n",
    "Total data from 15k to 30k.<br>\n",
    "Before/After Removing Stopwords.<br>\n",
    "\n",
    "\n",
    "At before and after of 15k and 30k after removing the stopwords the NB model has scaled better while only losing minimal accuracy in the evaluation metrics: precision, recall,f1 and accuracy.<br>\n",
    "\n",
    "Time taken to classify Data:<br>\n",
    "NB: The time taken to classify the data after the removal of stopwords has scaled well from 15k to 30k data at (0.0098 to 0.050s) as compared to before removal (0.0051 to 0.0099s). Which means that removing stopwords will help in scaling data as it help make the model scale the classification time of the tweets faster.\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB: As the number of total data increased from 15k to 30k while maintaining a small labelled data for training the precision, recall and accuracy has increased. \n",
    "\n",
    "Takeaway: This Naive Bayes with improved pre-processing has managed to improve the performance metrics of the model through the stopwords removal and scales well as the number of data to be process increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15k with stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "print(\"15k without stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "print(\"30k with stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "print(\"30k without stopwords-----------------\\n\")\n",
    "print(\"30k-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes scaling from 15k to 30k increasing MF\n",
    "Kept the Labelled data at 750.<br>\n",
    "Increasing the Max Features from 5000 to 7500 to 10000.<br>\n",
    "Total data from 15k to 30k.<br>\n",
    "After Removing Stopwords.<br>\n",
    "\n",
    "At 15k data set we have found that optimum MF = 7500 and at 30k MF = 10000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15k without stopwords MF increased-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,10000)\n",
    "print(\"30k without stopwords MF increased-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Naive Bayes performed the best and when scaling MF should be optimized (it might be increased or decreased) to find the best performance in terms of time and evaluation metrics.  At this example "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
