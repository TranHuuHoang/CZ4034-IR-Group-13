{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Pre-Processing: \n",
    "Stopwords are removed in the tweets in order to improve the performance of classification using Naive Bayes Model At line 25 using a predefined stopwords.Here we pre-process the 30k data. \n",
    "\n",
    "Stopwords-One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "df = tweets = pd.read_excel(\"./clean_healthcare30000.xlsx\")\n",
    "df = df[['TweetText','Polarity']]\n",
    "stop_words = set(['a','about','above','after','again','against','all','am','an','and','any','are','aren\\'t','as','at','be','because','been','before','being','below','between','both','but','by','can\\'t','cannot','could','couldn\\'t','did','didn\\'t','do','does','doesn\\'t','doing','don\\'t','down','during','each','few','for','from','further','had','hadn\\'t','has','hasn\\'t','have','haven\\'t','having','he','he\\'d','he\\'ll','he\\'s','her','here','here\\'s','hers','herself','him','himself','his','how','how\\'s','i','i\\'d','i\\'ll','i\\'m','i\\'ve','if','in','into','is','isn\\'t','it','it\\'s','its','itself','let\\'s','me','more','most','mustn\\'t','my','myself','no','nor','not','of','off','on','once','only','or','other','ought','our','ours', 'ourselves','out','over','own','same','shan\\'t','she','she\\'d','she\\'ll','she\\'s','should','shouldn\\'t','so','some','such','than','that','that\\'s','the','their','theirs','them','themselves','then','there','there\\'s','these','they','they\\'d','they\\'ll','they\\'re','they\\'ve','this','those','through','to','too','under','until','up','very','was','wasn\\'t','we','we\\'d','we\\'ll','we\\'re','we\\'ve','were','weren\\'t','what','what\\'s','when','when\\'s','where','where\\'s','which','while','who','who\\'s','whom','why','why\\'s','with','won\\'t','would','wouldn\\'t','you','you\\'d','you\\'ll','you\\'re','you\\'ve','your','yours','yourself','yourselves'])\n",
    "\n",
    "def processRow(row):\n",
    "    tweet = row.lower()    #Lower case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',tweet)    #delete any url\n",
    "    tweet = re.sub('@[^\\s]+','',tweet) #delete any @Username\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)#Remove additional white spaces\n",
    "    tweet = re.sub('[\\n]+', ' ', tweet) #Remove not alphanumeric symbols white spaces\n",
    "    tweet = re.sub(r'[^\\w]', ' ', tweet) #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #Remove Digits\n",
    "    tweet = re.sub(\" \\d+\", '', tweet)\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    tweet = tweet.replace(':)','')    #Remove :( or :)\n",
    "    tweet = tweet.replace(':(','')\n",
    "    tweet = tweet.strip('\\'\"')    #trim\n",
    "    tweet = [word for word in tweet.split() if word not in stop_words and len(word) > 1]#Removes stopwords and single letter words\n",
    "    return ''.join(str(e) + \" \" for e in tweet)\n",
    "testing = np.array(list(df['TweetText'][:30000]))  \n",
    "for x in range(0,testing.shape[0]):\n",
    "    testing[x] = processRow(testing[x])\n",
    "df.TweetText = testing\n",
    "\n",
    "#df.to_csv(\"clean_stopwordsremoved_healthcaretweet30000.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models: Naive Bayes (NB)/ Support Vector Machine (SVM)/ Logistic Regression (LR)\n",
    "This is the code used to get he results excel sheet to compare the performance of the three models in order to choose the fastest and most accurate. The model chosen (NB) as it is the fastest and performed the best in these two criteria.\n",
    "In order to use the code you should have the nltk and sklearn packages.<br>\n",
    "\n",
    "The Evaluation Metrics used to test the models performance are skllearns: precision, recall, fscore(f1), and accuracy.\n",
    "The Performance Metrics use to thes the models performance are in the terms of time taken to Build the Text Corpus, Vectorize the data and for the models to classify the overall record (15k/30k data).<br>\n",
    "\n",
    "Naming Concention of the csv files:<br>\n",
    "CSV files with stopwords:<br>\n",
    "    Version1: 15k data set<br>\n",
    "    clean_healthcaretweet750 - 750 labelled of 15k<br>\n",
    "    clean_healthcaretweet1500 - 1500 labelled of 15k<br>\n",
    "    clean_healthcaretweet2250 - 2250 labelled of 15k<br>\n",
    "    clean_healthcaretweet3000 - 3000 labelled of 15k<br>\n",
    "    Version2: 30k data set<br>\n",
    "    clean_healthcaretweet750_30k - 750 labelled of 15k<br>\n",
    "    clean_healthcaretweet1500_30k - 1500 labelled of 15k<br>\n",
    "    clean_healthcaretweet2250_30k - 2250 labelled of 15k<br>\n",
    "    clean_healthcaretweet3000_30k - 3000 labelled of 15k<br>\n",
    "\n",
    "CSV files without stopwords:<br>\n",
    "    Version1: 15k data set<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_15k - 750 labelled of 15k<br>\n",
    "    clean_stopwordsremoved_healthcaretweet1500_15k - 1500 labelled of 15k<br>\n",
    "    Version2: 30k data set<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_30k - 750 labelled of 30k<br>\n",
    "    clean_stopwordsremoved_healthcaretweet750_30k - 1500 labelled of 30k<br>\n",
    "Below is the code used to test different performance evaluations of the three models: test set=20%   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set Random seed\n",
    "np.random.seed(500)\n",
    "\n",
    "\n",
    "def calPerformanceofModels(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "    start = time.time()\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    end = time.time()\n",
    "    tok_time = end-start\n",
    "    \n",
    "    # Split the model into Train and Test Data set\n",
    "    start  = time.time()\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "    # Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    # Now we can run different algorithms to classify our data check for accuracy\n",
    "    end  = time.time()\n",
    "    vect_time = end-start\n",
    "    # Classifier - Algorithm - Naive Bayes\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    \n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_NB = Naive.predict(Test_X_Tfidf) # predict the labels on validation dataset\n",
    "    \n",
    "    end  = time.time()\n",
    "    nb_time = end -start\n",
    "    #NAIVE BAYES END\n",
    "    # Classifier - Algorithm - SVM\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time()\n",
    "    \n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_SVM = SVM.predict(Test_X_Tfidf)    # predict the labels on validation dataset\n",
    "    \n",
    "    end  = time.time()  \n",
    "    svm_time = end -start\n",
    "    #SVM END\n",
    "    \n",
    "    # Classifier - Algorithm - Logistic Regression\n",
    "    # fit the training dataset on the classifier\n",
    "    start  = time.time() \n",
    "    \n",
    "    LogReg = LogisticRegression()\n",
    "    LogReg.fit(Train_X_Tfidf,Train_Y)\n",
    "    predictions_LR = LogReg.predict(Test_X_Tfidf) # predict the labels on validation dataset\n",
    "    \n",
    "    end = time.time()\n",
    "    lr_time = end -start\n",
    "    #lOGISTIC REGRESSION END\n",
    "\n",
    "    #We compute the precison, recall and fscore of each of the models\n",
    "    #We use Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for\n",
    "    #label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "    prf_NB = precision_recall_fscore_support(predictions_NB, Test_Y,average='weighted')\n",
    "    prf_SVM = precision_recall_fscore_support(predictions_SVM, Test_Y,average='weighted')\n",
    "    prf_LR = precision_recall_fscore_support(predictions_LR, Test_Y,average='weighted')\n",
    "    print(\"~~~~For N labels = \", label , \", Max features = \", max_features  ,\"~~~~~\\n\")\n",
    "    print(\"===Naive Bayes===\\nPrecision, Recall, F1-Score: \",  prf_NB[0]*100,prf_NB[1]*100,prf_NB[2]*100 )\n",
    "    print(\"Naive Bayes Accuracy Score -> \",accuracy_score(Test_Y, predictions_NB)*100)\n",
    "    print(\"===SVM===\\nPrecision, Recall, F1-Score: \",  prf_SVM[0]*100,prf_SVM[1]*100,prf_SVM[2]*100)\n",
    "    print(\"SVM Accuracy Score -> \",accuracy_score(Test_Y, predictions_SVM)*100)\n",
    "    print(\"===Logistic Regression===\\nPrecision, Recall, F1-Score: \",  prf_LR[0]*100,prf_LR[1]*100,prf_LR[2]*100)\n",
    "    print(\"LR Accuracy Score -> \",accuracy_score(Test_Y,predictions_LR )*100)\n",
    "    print(\"Time spent on tokenizing for bag of words: \", tok_time, \"\\n\")\n",
    "    print(\"Time spent on vectorizing for NB/SVM/LR\", vect_time, \"\\n\")\n",
    "    print(\"Time spent on predicting for NB/SVM/LR models (respectively): \", \"\\n\",\n",
    "          nb_time, \"/\", svm_time ,\"/\", lr_time,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests Conducted: \n",
    "Variables being changed Max Features (number of important words in the text Corpus), Number of labelled Data and Total Number of Data (Scaling).<br>\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.<br>\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.<br>\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0. In our example we used a multiplier of 100 so as we can read it clearer.<br>\n",
    "\n",
    "The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1 (15k data set): There are Three variables that we vary to observe the performance of the three models: NB, SVM, LB \n",
    "For the first experiment we kept the Max Features and the total data the same.\n",
    "Changing the labelled Data.<br>\n",
    "\n",
    "Conclusion: Results is that as more labelled data is trained for classifying the 15k sets of tweets the evaluation metrics precision.recall/f1 and accuracy decreased significantly. However, a good effect is that the tokenizing time has decreased by roughly 5%. The time spent on vectorizing stayed roughly the same while classifying time for 15k data set has decreased as more labelled data is fed to it. \n",
    "\n",
    "Best to worst performing Model(evaluation and performance metrics): NB>>LB>>SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  750 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.79491584852735 94.96666666666667 97.32094375106858\n",
      "Naive Bayes Accuracy Score ->  94.96666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 95.06666666666666 97.47095010252905\n",
      "SVM Accuracy Score ->  95.06666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.06666666666666 97.47095010252905\n",
      "LR Accuracy Score ->  95.06666666666666\n",
      "Time spent on tokenizing for bag of words:  184.51742959022522 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5874297618865967 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.033376455307006836 / 8.49254322052002 / 0.08976078033447266 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.83350427350427 79.73333333333333 88.63810471087804\n",
      "Naive Bayes Accuracy Score ->  79.73333333333333\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.93350427350427 79.73333333333333 88.6774973080715\n",
      "SVM Accuracy Score ->  79.73333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 79.7 88.70339454646633\n",
      "LR Accuracy Score ->  79.7\n",
      "Time spent on tokenizing for bag of words:  176.07911324501038 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.4617650508880615 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0030002593994140625 / 19.207633018493652 / 0.1146550178527832 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.79590666226004 85.0 91.74997542722414\n",
      "Naive Bayes Accuracy Score ->  85.0\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 84.93333333333334 91.85291997116076\n",
      "SVM Accuracy Score ->  84.93333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 84.93333333333334 91.85291997116076\n",
      "LR Accuracy Score ->  84.93333333333334\n",
      "Time spent on tokenizing for bag of words:  175.8518829345703 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.49367666244506836 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0029914379119873047 / 16.90679907798767 / 0.1126565933227539 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.89221951009529 80.73333333333333 89.27652856002246\n",
      "Naive Bayes Accuracy Score ->  80.73333333333333\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.96666666666667 80.73333333333333 89.32642193937158\n",
      "SVM Accuracy Score ->  80.73333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 80.73333333333333 89.33972703799337\n",
      "LR Accuracy Score ->  80.73333333333333\n",
      "Time spent on tokenizing for bag of words:  174.98588633537292 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5335743427276611 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003988981246948242 / 19.918781757354736 / 0.1106572151184082 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3 (30k data set): There are Three variables that we vary to observe the performance of the three models: NB, SVM, LB \n",
    "For the third experiment we kept the Max Features and the total data the same.\n",
    "Changing the labelled Data.<br>\n",
    "Conclusion: Results is that as more labelled data is trained for classifying the 30k sets of tweets the evaluation metrics precision.recall/f1 and accuracy decreased roughly 4-7%. Moreover, the tokenizing time has increased roughly twice from 500s to 1000s. The time spent on vectorizing almost doubled as well. <br><br>\n",
    "Performance metrics Classification time For Models (increase of labelled data from 750 to 3000 of 30k):<br>\n",
    "NB: Decreased as labelled data increased<br>\n",
    "SVM: Increased 10x as labbeled data increased <br>\n",
    "LB: Stayed Roughly the Same<br>\n",
    "Classification time For Models (increase of labelled data from 750 to 3000 of 30k):<br>\n",
    "NB: Decreased as labelled data increased<br>\n",
    "SVM: Increased 10x as labbeled data increased <br>\n",
    "LB: Stayed Roughly the Same<br>\n",
    "\n",
    "Best to worst performing Model : NB>>LB>>SVM<br>\n",
    "In terms of time NB performed the fastest and the most accurate while LB/SVM performed around the same accuracy but worse than NB. SVM performed significantly slower than NB and LB and shouldn't used for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  750 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 97.41666666666666 98.69143098353736\n",
      "Naive Bayes Accuracy Score ->  97.41666666666666\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 97.41666666666666 98.69143098353736\n",
      "SVM Accuracy Score ->  97.41666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 97.41666666666666 98.69143098353736\n",
      "LR Accuracy Score ->  97.41666666666666\n",
      "Time spent on tokenizing for bag of words:  476.29352617263794 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.01922607421875 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006007194519042969 / 21.38977885246277 / 0.22838854789733887 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.8807737260402 95.01666666666667 97.38802279202281\n",
      "Naive Bayes Accuracy Score ->  95.01666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98333333333333 95.06666666666666 97.46303227092767\n",
      "SVM Accuracy Score ->  95.06666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.06666666666666 97.47095010252905\n",
      "LR Accuracy Score ->  95.06666666666666\n",
      "Time spent on tokenizing for bag of words:  474.59759187698364 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.035231113433838 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006981372833251953 / 41.90494465827942 / 0.251326322555542 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.87915913745331 92.18333333333332 95.87706273489448\n",
      "Naive Bayes Accuracy Score ->  92.18333333333332\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98333333333333 92.23333333333333 95.95209688141276\n",
      "SVM Accuracy Score ->  92.23333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 92.23333333333333 95.95977111149644\n",
      "LR Accuracy Score ->  92.23333333333333\n",
      "Time spent on tokenizing for bag of words:  467.78125858306885 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9843709468841553 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.004990339279174805 / 61.96637582778931 / 0.2762291431427002 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.98333333333333 89.93333333333334 94.69242065233288\n",
      "Naive Bayes Accuracy Score ->  89.93333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98333333333333 89.93333333333334 94.69242065233288\n",
      "SVM Accuracy Score ->  89.93333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 89.93333333333334 94.6998946998947\n",
      "LR Accuracy Score ->  89.93333333333334\n",
      "Time spent on tokenizing for bag of words:  484.4093222618103 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.981386661529541 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.004985809326171875 / 92.6781313419342 / 0.36108875274658203 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250_30k.csv',2250,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 (15k data set): Vary labelled data and Num of Max Features (MF), Keep total data the same.\n",
    "Changed the Labelled data from 750,1500,2250 and 3000 respectively.<br>\n",
    "Changed the Max Features from 2500,5000,7500 and 10000 respectively.<br>\n",
    "The data will load qite slow roughly 400s to 1000s per call of calPerformanceofModels:<br>\n",
    "\n",
    "Increasing MF increased the time it takes to tokenize, and vectorize the data for the same total number of data. From the test the MF value of roughly 5000 performed better thus 5000 was chosen for scaling.<br>\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR:Tokenizing and Vectorizing increased more than double<br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased x4 roughly<br>\n",
    "SVM:  Increased x4 roughly<br>\n",
    "LR: Increased x3 roughly<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 10%-15% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  750 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.96666666666667 94.76666666666667 97.29723838867969\n",
      "Naive Bayes Accuracy Score ->  94.76666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.93544973544974 94.83333333333334 97.28554980487183\n",
      "SVM Accuracy Score ->  94.83333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 94.76666666666667 97.31302413143933\n",
      "LR Accuracy Score ->  94.76666666666667\n",
      "Time spent on tokenizing for bag of words:  181.5881769657135 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5404908657073975 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.002992391586303711 / 8.721717119216919 / 0.09670186042785645 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 95.76666666666667 97.83756172313979\n",
      "Naive Bayes Accuracy Score ->  95.76666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 95.76666666666667 97.83756172313979\n",
      "SVM Accuracy Score ->  95.76666666666667\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.76666666666667 97.83756172313979\n",
      "LR Accuracy Score ->  95.76666666666667\n",
      "Time spent on tokenizing for bag of words:  189.08103394508362 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5036575794219971 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0029916763305664062 / 8.10434889793396 / 0.09273266792297363 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 95.16666666666667 97.52348420153714\n",
      "Naive Bayes Accuracy Score ->  95.16666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.96728395061729 95.19999999999999 97.50884223167789\n",
      "SVM Accuracy Score ->  95.19999999999999\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.16666666666667 97.52348420153714\n",
      "LR Accuracy Score ->  95.16666666666667\n",
      "Time spent on tokenizing for bag of words:  193.9614429473877 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.6084010601043701 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.008248090744018555 / 10.491409540176392 / 0.14686918258666992 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 95.36666666666666 97.62839105954615\n",
      "Naive Bayes Accuracy Score ->  95.36666666666666\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.96729559748428 95.39999999999999 97.61373741204228\n",
      "SVM Accuracy Score ->  95.39999999999999\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.36666666666666 97.62839105954615\n",
      "LR Accuracy Score ->  95.36666666666666\n",
      "Time spent on tokenizing for bag of words:  213.24469017982483 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5797088146209717 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003994464874267578 / 10.83935832977295 / 0.1236579418182373 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.79158518807208 78.43333333333334 87.7919796912129\n",
      "Naive Bayes Accuracy Score ->  78.43333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.79143898924167 78.43333333333334 87.79169360049202\n",
      "SVM Accuracy Score ->  78.43333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  99.92416383219955 78.36666666666666 87.84213542640005\n",
      "LR Accuracy Score ->  78.36666666666666\n",
      "Time spent on tokenizing for bag of words:  190.60043811798096 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5754613876342773 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.002991914749145508 / 19.93942165374756 / 0.12566447257995605 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 80.80000000000001 89.38053097345133\n",
      "Naive Bayes Accuracy Score ->  80.80000000000001\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 80.80000000000001 89.38053097345133\n",
      "SVM Accuracy Score ->  80.80000000000001\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 80.80000000000001 89.38053097345133\n",
      "LR Accuracy Score ->  80.80000000000001\n",
      "Time spent on tokenizing for bag of words:  202.09008812904358 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.630307674407959 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0029935836791992188 / 18.871244430541992 / 0.1116631031036377 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.81733553992554 80.5 89.1239379386775\n",
      "Naive Bayes Accuracy Score ->  80.5\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.89198731209488 80.53333333333333 89.17412126808249\n",
      "SVM Accuracy Score ->  80.53333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 80.56666666666666 89.23758537936126\n",
      "LR Accuracy Score ->  80.56666666666666\n",
      "Time spent on tokenizing for bag of words:  191.08657598495483 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5086488723754883 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003947734832763672 / 22.315194129943848 / 0.11568975448608398 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 79.26666666666667 88.43436221643734\n",
      "Naive Bayes Accuracy Score ->  79.26666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.92462853938883 79.23333333333333 88.38414233463519\n",
      "SVM Accuracy Score ->  79.23333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 79.26666666666667 88.43436221643734\n",
      "LR Accuracy Score ->  79.26666666666667\n",
      "Time spent on tokenizing for bag of words:  208.05557131767273 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5764589309692383 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.004986763000488281 / 23.943338871002197 / 0.13563799858093262 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.79529157494038 84.3 91.35856142527125\n",
      "Naive Bayes Accuracy Score ->  84.3\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.90036231884059 84.3 91.4205177508555\n",
      "SVM Accuracy Score ->  84.3\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 84.26666666666667 91.46164978292329\n",
      "LR Accuracy Score ->  84.26666666666667\n",
      "Time spent on tokenizing for bag of words:  197.30562925338745 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5734286308288574 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.009974241256713867 / 17.479942083358765 / 0.11967825889587402 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 84.23333333333333 91.4420119413787\n",
      "Naive Bayes Accuracy Score ->  84.23333333333333\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 84.23333333333333 91.4420119413787\n",
      "SVM Accuracy Score ->  84.23333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 84.23333333333333 91.4420119413787\n",
      "LR Accuracy Score ->  84.23333333333333\n",
      "Time spent on tokenizing for bag of words:  189.6792323589325 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5196108818054199 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003989219665527344 / 15.090664625167847 / 0.09672331809997559 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.78228593872741 84.76666666666667 91.66361176258344\n",
      "Naive Bayes Accuracy Score ->  84.76666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.93351548269581 84.89999999999999 91.78617280465107\n",
      "SVM Accuracy Score ->  84.89999999999999\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 84.86666666666667 91.81391994230076\n",
      "LR Accuracy Score ->  84.86666666666667\n",
      "Time spent on tokenizing for bag of words:  185.502610206604 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.49068784713745117 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003989458084106445 / 18.34106421470642 / 0.13962578773498535 \n",
      "\n",
      "~~~~For N labels =  2250 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.92743328100471 84.89999999999999 91.80281233098971\n",
      "Naive Bayes Accuracy Score ->  84.89999999999999\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.9 84.93333333333334 91.81071235347162\n",
      "SVM Accuracy Score ->  84.93333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 84.93333333333334 91.85291997116076\n",
      "LR Accuracy Score ->  84.93333333333334\n",
      "Time spent on tokenizing for bag of words:  186.79245352745056 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.8228001594543457 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.005983114242553711 / 19.236602544784546 / 0.153548002243042 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  3000 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.81533898305085 78.60000000000001 87.94631323501959\n",
      "Naive Bayes Accuracy Score ->  78.60000000000001\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.93333333333332 78.66666666666666 88.03384347393306\n",
      "SVM Accuracy Score ->  78.66666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 78.66666666666666 88.0597014925373\n",
      "LR Accuracy Score ->  78.66666666666666\n",
      "Time spent on tokenizing for bag of words:  181.80880045890808 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.47769689559936523 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.004003763198852539 / 19.35619616508484 / 0.12367010116577148 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.925306854227 80.53333333333333 89.18739537173805\n",
      "Naive Bayes Accuracy Score ->  80.53333333333333\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.925306854227 80.53333333333333 89.18739537173805\n",
      "SVM Accuracy Score ->  80.53333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 80.56666666666666 89.23758537936126\n",
      "LR Accuracy Score ->  80.56666666666666\n",
      "Time spent on tokenizing for bag of words:  185.35129165649414 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5115931034088135 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003989696502685547 / 19.013198375701904 / 0.10072445869445801 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.77737315875615 81.36666666666666 89.63642712736629\n",
      "Naive Bayes Accuracy Score ->  81.36666666666666\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.96680731364275 81.5 89.77376287277725\n",
      "SVM Accuracy Score ->  81.5\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  99.96680731364275 81.5 89.77376287277725\n",
      "LR Accuracy Score ->  81.5\n",
      "Time spent on tokenizing for bag of words:  185.108491897583 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5255956649780273 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.003991127014160156 / 21.254128456115723 / 0.15558171272277832 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.93362013667875 80.0 88.82196971586428\n",
      "Naive Bayes Accuracy Score ->  80.0\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.85862276944533 79.93333333333334 88.77175577552613\n",
      "SVM Accuracy Score ->  79.93333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 79.93333333333334 88.84772137828826\n",
      "LR Accuracy Score ->  79.93333333333334\n",
      "Time spent on tokenizing for bag of words:  185.11565041542053 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.5864341259002686 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.007976055145263672 / 23.2239670753479 / 0.16456389427185059 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#750 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750, 10000)\n",
    "#1500 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500.csv',1500, 10000)\n",
    "#2250 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet2250.csv',2250, 10000)\n",
    "#3000 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000.csv',3000, 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4 (30k data set): Vary labelled data and Num of Max Features (MF), Keep total data the same. Testing on Larger Data.\n",
    "Changed the Labelled data from 750,1500 and 3000 respectively.<br>\n",
    "Changed the Max Features from 2500,5000,7500 and 10000 respectively.<br>\n",
    "\n",
    "The difference between 15k and 30k data is that at 30k data the time taken to tokenize it stayed relatively the same as MF is increased which is good for scaling.\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR:Tokenizing and Vectorizing increased more than double<br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased x4 roughly<br>\n",
    "SVM:  Increased x4 roughly<br>\n",
    "LR: Increased x3 roughly<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 10%-15% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br><br>\n",
    "Takeaway: at scaling number of labelled data matters, a lower number might be better as biasedness in data might be better for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~For N labels =  750 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 97.26666666666667 98.6143967556607\n",
      "Naive Bayes Accuracy Score ->  97.26666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 97.26666666666667 98.6143967556607\n",
      "SVM Accuracy Score ->  97.26666666666667\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 97.26666666666667 98.6143967556607\n",
      "LR Accuracy Score ->  97.26666666666667\n",
      "Time spent on tokenizing for bag of words:  545.0701026916504 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.653580904006958 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.008974313735961914 / 30.80861496925354 / 0.4667847156524658 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.8986733995207 97.31666666666666 98.59076780133456\n",
      "Naive Bayes Accuracy Score ->  97.31666666666666\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 97.36666666666667 98.66576591791927\n",
      "SVM Accuracy Score ->  97.36666666666667\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 97.36666666666667 98.66576591791927\n",
      "LR Accuracy Score ->  97.36666666666667\n",
      "Time spent on tokenizing for bag of words:  553.5694665908813 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9604320526123047 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.005990505218505859 / 19.83392310142517 / 0.22839093208312988 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.89859420538316 97.16666666666667 98.51369399830939\n",
      "Naive Bayes Accuracy Score ->  97.16666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98358585858587 97.23333333333333 98.58108969183832\n",
      "SVM Accuracy Score ->  97.23333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 97.21666666666667 98.58869263922927\n",
      "LR Accuracy Score ->  97.21666666666667\n",
      "Time spent on tokenizing for bag of words:  493.08871579170227 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9644613265991211 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.005944252014160156 / 21.148478507995605 / 0.29418206214904785 \n",
      "\n",
      "~~~~For N labels =  750 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.96626841895659 97.63333333333334 98.78602912239278\n",
      "Naive Bayes Accuracy Score ->  97.63333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 97.65 98.81102959777385\n",
      "SVM Accuracy Score ->  97.65\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 97.65 98.81102959777385\n",
      "LR Accuracy Score ->  97.65\n",
      "Time spent on tokenizing for bag of words:  464.7916593551636 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.0042991638183594 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006989002227783203 / 22.896722555160522 / 0.26429200172424316 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 94.68333333333334 97.26906942898725\n",
      "Naive Bayes Accuracy Score ->  94.68333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 94.68333333333334 97.26906942898725\n",
      "SVM Accuracy Score ->  94.68333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 94.68333333333334 97.26906942898725\n",
      "LR Accuracy Score ->  94.68333333333334\n",
      "Time spent on tokenizing for bag of words:  450.446870803833 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9554152488708496 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.004986286163330078 / 28.481881380081177 / 0.2004690170288086 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.98347107438018 95.13333333333334 97.48957500660536\n",
      "Naive Bayes Accuracy Score ->  95.13333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.95013774104683 95.13333333333334 97.47372674420775\n",
      "SVM Accuracy Score ->  95.13333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 95.11666666666667 97.49722388314683\n",
      "LR Accuracy Score ->  95.11666666666667\n",
      "Time spent on tokenizing for bag of words:  663.6797389984131 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.010298728942871 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.00598454475402832 / 39.695887327194214 / 0.2463386058807373 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.96666666666667 94.73333333333333 97.27966672373452\n",
      "Naive Bayes Accuracy Score ->  94.73333333333333\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.93280617742788 94.73333333333333 97.2554247747533\n",
      "SVM Accuracy Score ->  94.73333333333333\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 94.73333333333333 97.29544676480658\n",
      "LR Accuracy Score ->  94.73333333333333\n",
      "Time spent on tokenizing for bag of words:  445.9431025981903 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9724056720733643 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.005975008010864258 / 50.95072627067566 / 0.2812488079071045 \n",
      "\n",
      "~~~~For N labels =  1500 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.96572443348597 94.61666666666667 97.21767291827238\n",
      "Naive Bayes Accuracy Score ->  94.61666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98333333333333 94.63333333333334 97.23479775056379\n",
      "SVM Accuracy Score ->  94.63333333333334\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 94.63333333333334 97.24267854084603\n",
      "LR Accuracy Score ->  94.63333333333334\n",
      "Time spent on tokenizing for bag of words:  450.50156259536743 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9834034442901611 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006946086883544922 / 53.12398958206177 / 0.32213902473449707 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  2500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 89.64999999999999 94.54257843395727\n",
      "Naive Bayes Accuracy Score ->  89.64999999999999\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  100.0 89.64999999999999 94.54257843395727\n",
      "SVM Accuracy Score ->  89.64999999999999\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 89.64999999999999 94.54257843395727\n",
      "LR Accuracy Score ->  89.64999999999999\n",
      "Time spent on tokenizing for bag of words:  472.0037565231323 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9663918018341064 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006015300750732422 / 65.64044380187988 / 0.29122114181518555 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  5000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  100.0 90.08333333333334 94.7829899167032\n",
      "Naive Bayes Accuracy Score ->  90.08333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.94817144619181 90.06666666666666 94.75048086760792\n",
      "SVM Accuracy Score ->  90.06666666666666\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 90.08333333333334 94.7829899167032\n",
      "LR Accuracy Score ->  90.08333333333334\n",
      "Time spent on tokenizing for bag of words:  470.56253266334534 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.0531952381134033 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0059566497802734375 / 99.3824634552002 / 0.2812473773956299 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  7500 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.93170569498896 90.43333333333334 94.93656332900885\n",
      "Naive Bayes Accuracy Score ->  90.43333333333334\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.98339646464646 90.45 94.96897771905921\n",
      "SVM Accuracy Score ->  90.45\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 90.43333333333334 94.97636968317872\n",
      "LR Accuracy Score ->  90.43333333333334\n",
      "Time spent on tokenizing for bag of words:  482.2127664089203 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 1.1588969230651855 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.0059490203857421875 / 123.50887250900269 / 0.35408592224121094 \n",
      "\n",
      "~~~~For N labels =  3000 , Max features =  10000 ~~~~~\n",
      "\n",
      "===Naive Bayes===\n",
      "Precision, Recall, F1-Score:  99.86141390810987 89.91666666666667 94.6196836515267\n",
      "Naive Bayes Accuracy Score ->  89.91666666666667\n",
      "===SVM===\n",
      "Precision, Recall, F1-Score:  99.96681034482758 89.96666666666667 94.69446655890711\n",
      "SVM Accuracy Score ->  89.96666666666667\n",
      "===Logistic Regression===\n",
      "Precision, Recall, F1-Score:  100.0 89.95 94.70913398262701\n",
      "LR Accuracy Score ->  89.95\n",
      "Time spent on tokenizing for bag of words:  468.77095890045166 \n",
      "\n",
      "Time spent on vectorizing for NB/SVM/LR 0.9684543609619141 \n",
      "\n",
      "Time spent on predicting for NB/SVM/LR models (respectively):  \n",
      " 0.006938457489013672 / 133.99350547790527 / 0.36801600456237793 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#750 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,10000)\n",
    "#1500 labelled data, change MF \n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,10000)\n",
    "#2250 labelled data, change MF \n",
    "## Validate Results using clean_healthcaretweet1500 dataset\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,2500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,7500)\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,10000)\n",
    "#3000 labelled data, change MF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5 (30k data set): Effect of Removing Stopwords on Data, on the Three Models.\n",
    "Changed the Labelled data from 750,1500 and 3000 respectively.<br>\n",
    "Kept the Max Features at 5000.<br>\n",
    "\n",
    "The difference between 15k and 30k data is that at 30k data the time taken to tokenize it stayed relatively the same as MF is increased which is good for scaling.\n",
    "\n",
    "Time taken to tokenize and Vectorize data:<br>\n",
    "NB/SVM/LR: After stopwords has been removed time take to tokenize and has signigicantly decreased to more than half (860s to 320s) while time taken to vectorize data increased as the number of labelled data and after removing stopwords from (0.98s to 1.03s) which is not very big. <br>\n",
    "Time taken to classify Data:<br>\n",
    "NB: Increased (from 0.0059s to 0.0069) as labelled data increased. Thus it is better to keep labelled data a bit smaller. <br>\n",
    "SVM/LR:  For time to classify data both these models stayed almost the same with/without stopwords<br>\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB/SVM/LR: As labelled data increased from 750 to 3000 at the same MF it decreased significantly (roughly 4-7% decrease), However increasing the MF together with the labelled data alleviate the decrease performance by the model.  <br>\n",
    "Best to worst Performing Model (time taken/performance metrics): NB>>LB>>SVM<br><br>\n",
    "\n",
    "\n",
    "Takeaway: Throughout all the test NB performed the best in terms of both time and performance and in terms of possible scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before-----------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-77893367bbaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#750\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Before-----------------\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcalPerformanceofModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./clean_healthcaretweet750_30k.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After-----------------\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcalPerformanceofModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./clean_stopwordsremoved_healthcaretweet750_30k.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m750\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-77c6399725b9>\u001b[0m in \u001b[0;36mcalPerformanceofModels\u001b[1;34m(path, label, max_features)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;31m# Below condition is to check for Stop words and consider only alphabets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mword_Final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_Lemmatized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mFinal_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_Final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     23\u001b[0m         return [\n\u001b[0;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#750\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "#1500\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet1500_30k.csv',1500,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet1500_30k.csv',1500,5000)\n",
    "#3000\n",
    "print(\"Before-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet3000_30k.csv',3000,5000)\n",
    "print(\"After-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet3000_30k.csv',3000,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes scaling from 15k to 30k removing Stopwords\n",
    "Kept the Labelled data at 750.<br>\n",
    "Kept the Max Features 5000.<br>\n",
    "Total data from 15k to 30k.<br>\n",
    "Before/After Removing Stopwords.<br>\n",
    "\n",
    "\n",
    "At before and after of 15k and 30k after removing the stopwords the NB model has scaled better while only losing minimal accuracy in the evaluation metrics: precision, recall,f1 and accuracy.<br>\n",
    "\n",
    "Time taken to classify Data:<br>\n",
    "NB: The time taken to classify the data after the removal of stopwords has scaled well from 15k to 30k data at (0.0098 to 0.050s) as compared to before removal (0.0051 to 0.0099s). Which means that removing stopwords will help in scaling data as it help make the model scale the classification time of the tweets faster.\n",
    "\n",
    "Performarmance Metrics for models:\n",
    "NB: As the number of total data increased from 15k to 30k while maintaining a small labelled data for training the precision, recall and accuracy has increased. \n",
    "\n",
    "Takeaway: This Naive Bayes with improved pre-processing has managed to improve the performance metrics of the model through the stopwords removal and scales well as the number of data to be process increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15k with stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750.csv',750,5000)\n",
    "print(\"15k without stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "print(\"30k with stopwords-----------------\\n\")\n",
    "calPerformanceofModels('./clean_healthcaretweet750_30k.csv',750,5000)\n",
    "print(\"30k without stopwords-----------------\\n\")\n",
    "print(\"30k-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes scaling from 15k to 30k increasing MF\n",
    "Kept the Labelled data at 750.<br>\n",
    "Increasing the Max Features from 5000 to 7500 to 10000.<br>\n",
    "Total data from 15k to 30k.<br>\n",
    "After Removing Stopwords.<br>\n",
    "\n",
    "At 15k data set we have found that optimum MF = 7500 and at 30k MF = 10000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"15k without stopwords MF increased-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_15k.csv',750,10000)\n",
    "print(\"30k without stopwords MF increased-----------------\\n\")\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,5000)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,7500)\n",
    "calPerformanceofModels('./clean_stopwordsremoved_healthcaretweet750_30k.csv',750,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway:\n",
    "Naive Bayes performed the best and when scaling MF should be optimized (it might be increased or decreased) to find the best performance in terms of time and evaluation metrics.  At this example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part: Ensemble exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set Random seed\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and Vectorize (Train and Split Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Step - 1a : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    # Step - 2: Split the model into Train and Test Data set\n",
    "    Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "\n",
    "    # Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Step - 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    return Train_X_Tfidf, Test_X_Tfidf, Train_Y,Test_Y\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,y y_test = data('clean_healthcaretweet3000_30k.csv', 750,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models and Ensemble Performance\n",
    "Naive Bayes  <br />\n",
    "Model1 - Logistic Regression  <br />\n",
    "Model2 - Decisition Tree Classifier  <br />\n",
    "Model3 - Random Forest Classifier  <br />\n",
    "Model4 - Support Vector Machine  <br />\n",
    "Model5 - K Nearest Neighbour <br />\n",
    "As we can see from the performance of the models and the ensembles below, Naive Bayes still performed well enough to be <br />\n",
    "comparable with the other Ensemble models. There is almost no improvement when comparing the results.\n",
    "We used MaxVoting Models to figure out which models would perform the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxVotingmodels(x_train, x_test, y_train, y_test ):\n",
    "    #Models Used\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    model1 = LogisticRegression(random_state=1)\n",
    "    model2 = DecisionTreeClassifier( random_state=1)\n",
    "    model3 = RandomForestClassifier(n_estimators=500, random_state=1)\n",
    "    model4 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model5 = KNeighborsClassifier(n_neighbors=7)# KNN\n",
    "   \n",
    "    #knn\n",
    "    #boosted tree\n",
    "    #random forest\n",
    "    \n",
    "    #Voting classifiers, num indicates num of model inside\n",
    "    ens2 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),], voting='hard')\n",
    "    ens3 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive)], voting='hard')\n",
    "    ens4 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),], voting='hard')\n",
    "    ens5 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4)], voting='hard')\n",
    "    ens6 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4), ('knn', model5)], voting='hard')\n",
    " \n",
    "    Naive.fit(x_train,y_train)\n",
    "    model1.fit(x_train,y_train)\n",
    "    model2.fit(x_train,y_train)\n",
    "    model3.fit(x_train,y_train)\n",
    "    model4.fit(x_train,y_train)\n",
    "    model5.fit(x_train,y_train)\n",
    "    ens6.fit(x_train,y_train)\n",
    "    \n",
    "    print(\"6 model and all ensemble: Accuracy\")\n",
    "    for clf, label in zip([Naive, model1, model2,model3,model4,model5, ens2,ens3, ens4, ens5, ens6], ['Naive Bayes', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'Support Vector Machine', 'K-Nearest Neighbour', 'Ensemble2', 'Ensemble3', 'Ensemble4', 'Ensemble5', 'Ensemble6']):\n",
    "        scores = cross_val_score(clf, x_test, y_test, scoring='accuracy', cv=5)\n",
    "        print(\"Accuracy: %0.6f (+/- %0.6f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxVotingmodels(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data and Vectorize (Stratified K-fold Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stratified K-fold instead of train_test split for a more even data\n",
    "Lets try to see the effect of using stratified k-fold to balance an unbalanced data to be fed to the models<br />\n",
    "Accuracy(Before/After):  0.896167 0.905167 as we can see there is a slight improvement for Naive Bayes Classifier <br/>\n",
    "and the other models and Ensembles. This might be because the data is unbalanced.\n",
    "Suprisingly Logistic Regression faired better than Naive Bayes after the k-fold. \n",
    "\n",
    "Results: From the results below we can see improvements for the models and ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratifyData(path,label,max_features):\n",
    "    # Add the Data using pandas\n",
    "\n",
    "    Corpus = pd.read_csv(path,encoding='latin-1')\n",
    "    Corpus['Polarity'] = Corpus['Polarity'].apply(str) #converts the float string into string/obj for processing\n",
    "    Corpus.dropna()\n",
    "    #print(Corpus.shape)\n",
    "    # Step - 1a : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    Corpus['TweetText']= [word_tokenize(str(entry)) for entry in Corpus['TweetText']]\n",
    "    # Step - 1b: Perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    for index,entry in enumerate(Corpus['TweetText']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "    #print(Corpus['text_final'].head())\n",
    "    \n",
    "    # Step - 2: Split the model into Train and Test Data set\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    # X is the feature set and y is the target\n",
    "    for train_index, test_index in skf.split(X,y): \n",
    "        print(\"Train:\", train_index, \"Validation:\", val_index) \n",
    "        Train_X, Test_X = X[train_index], X[val_index] \n",
    "        Train_Y, Test_Y = y[train_index], y[val_index]\n",
    "    #Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['Polarity'],test_size=0.2)\n",
    "\n",
    "    # Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "    Encoder = LabelEncoder()\n",
    "    Train_Y = Encoder.fit_transform(Train_Y)\n",
    "    Test_Y = Encoder.fit_transform(Test_Y)\n",
    "    # Step - 4: Vectorize the words by using TF-IDF Vectorizer - This is done to find how important a word in document is in comaprison to the corpus\n",
    "    Tfidf_vect = TfidfVectorizer(max_features=max_features)\n",
    "    Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "    return Train_X_Tfidf, Test_X_Tfidf, Train_Y,Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified K-fold\n",
    "x_train1, x_test1, y_train1, y_test1 = data('clean_healthcaretweet3000_30k.csv', 750,5000)\n",
    "maxVotingmodels(x_train1, x_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter Tuning\n",
    "The reason why we chose random forest in order to be improved is because it is an ensemble bagging method which feature ramdomness when building each individual tree to create an uncorrelated subsets of data for less data biasedness. Thus it might result into a better and more accurate individual tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary Tuning: Randomized Search CV\n",
    "As we do not have a clue where to start in the tuning we started testing on a wide range of parameter values at a randomized way in order to help pinpoint the best parameter that would be later passed on to the final tuning with grid search cv which is a more thorough way of checking the best parameter given a specific set of values.\n",
    "\n",
    "Results: We managed to push the result of the Random Forest model from 90.35% to 90.55% however the caveat is that it took almost an hour to process it. If we wouldnt have randomized it it would have taken a much longer time to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n",
    "                               random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train1, y_train1)\n",
    "# Best Parameters for Random Forest\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = ',   accuracy_score(test_labels, predictions)*100)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 2000, min_samples_split = 2, min_samples_leaf =2 ,\n",
    "                                   max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "rf.fit(x_train1, y_train1)\n",
    "base_accuracy = evaluate(rf, x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondary Tuning: Randomized Search CV\n",
    "As we do not have a clue where to start in the tuning we started testing on a wide range of parameter values at a randomized way in order to help pinpoint the best parameter that would be later passed on to the final tuning with grid search cv which is a more thorough way of checking the best parameter given a specific set of values.\n",
    "\n",
    "Result: The results were a bit disappointing although expected. The results have capped at 90.55% for the model. This is also the cap for the rest of the models, with Linear Regression and Naive Bayes performing almost as well without parameter improvements. It also took around 2 hours to complete the final tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Further Tuning the parameter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [3, 4, 5],\n",
    "    'n_estimators': [100, 200, 300, 1000, 1500, 2250, 2500 ,3000]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1,\n",
    "                           verbose = 2)\n",
    "grid_search.fit(x_train1 ,y_train1)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, min_samples_leaf =3,\n",
    "                                   max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "rf.fit(x_train1, y_train1)\n",
    "base_accuracy = evaluate(rf, x_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxVotingmodels2(x_train, x_test, y_train, y_test ):\n",
    "    #Models Used\n",
    "    Naive = naive_bayes.MultinomialNB()\n",
    "    model1 = LogisticRegression(random_state=1)\n",
    "    model2 = DecisionTreeClassifier( random_state=1)\n",
    "    model3 = RandomForestClassifier(n_estimators = 100, min_samples_split = 5, min_samples_leaf =3,\n",
    "                                   max_features= 'auto',max_depth=90, bootstrap= True)\n",
    "    model4 = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    model5 = KNeighborsClassifier(n_neighbors=7)# KNN\n",
    "   \n",
    "    #knn\n",
    "    #boosted tree\n",
    "    #random forest\n",
    "    \n",
    "    #Voting classifiers, num indicates num of model inside\n",
    "    ens2 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),], voting='hard')\n",
    "    ens3 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive)], voting='hard')\n",
    "    ens4 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),], voting='hard')\n",
    "    ens5 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4)], voting='hard')\n",
    "    ens6 = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('nb', Naive), ('rf', model3),('svm', model4), ('knn', model5)], voting='hard')\n",
    " \n",
    "    Naive.fit(x_train,y_train)\n",
    "    model1.fit(x_train,y_train)\n",
    "    model2.fit(x_train,y_train)\n",
    "    model3.fit(x_train,y_train)\n",
    "    model4.fit(x_train,y_train)\n",
    "    model5.fit(x_train,y_train)\n",
    "    ens6.fit(x_train,y_train)\n",
    "    \n",
    "    print(\"6 model and all ensemble: Accuracy\")\n",
    "    for clf, label in zip([Naive, model1, model2,model3,model4,model5, ens2,ens3, ens4, ens5, ens6], ['Naive Bayes', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'Support Vector Machine', 'K-Nearest Neighbour', 'Ensemble2', 'Ensemble3', 'Ensemble4', 'Ensemble5', 'Ensemble6']):\n",
    "        scores = cross_val_score(clf, x_test, y_test, scoring='accuracy', cv=5)\n",
    "        print(\"Accuracy: %0.6f (+/- %0.6f) [%s]\" % (scores.mean(), scores.std(), label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxVotingmodels2(x_train1, x_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Thus for a simple sentimental analysis to figure out the sentiments of healthcare tweets, we would choose linear regression and Naive Bayes as the model for the job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
